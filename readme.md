# R√©seau de Neurones From Scratch en C++

Ce projet impl√©mente un r√©seau de neurones multicouche from scratch en C++ pour r√©soudre le probl√®me du XOR. L'impl√©mentation n'utilise aucune biblioth√®que externe de machine learning, seulement les biblioth√®ques standards C++.

## üß† Concept et Architecture

Le r√©seau est compos√© de :
- Couche d'entr√©e : 2 neurones (pour les 2 entr√©es du XOR)
- Couche cach√©e : 4 neurones
- Couche de sortie : 1 neurone

### Probl√®me du XOR

Le XOR (OU exclusif) est un probl√®me classique non lin√©airement s√©parable :
```
Entr√©e1  Entr√©e2  |  Sortie
   0       0      |    0
   0       1      |    1
   1       0      |    1
   1       1      |    0
```

## üîç D√©tails Techniques

### Fonction d'Activation

Initialement, la sigmo√Øde √©tait utilis√©e comme fonction d'activation :
```cpp
double activate(double x) {
    return 1.0 / (1.0 + exp(-x)); // Sortie entre [0, 1]
}
```

Probl√®me rencontr√© : La sigmo√Øde n'√©tait pas adapt√©e car :
- Elle est centr√©e autour de 0.5 et non de 0
- Sa plage de sortie [0, 1] n'est pas optimale pour le XOR
- Les gradients sont plus faibles que tanh

Solution : Utilisation de tanh :
```cpp
double activate(double x) {
    return tanh(x); // Sortie entre [-1, 1]
}
```

Avantages de tanh :
- Centr√©e en 0
- Plage de sortie [-1, 1]
- Gradients plus forts
- Meilleure convergence pour ce type de probl√®me

### Backpropagation

La backpropagation utilise le concept de delta (Œ¥) :
- Pour la couche de sortie : Œ¥ = (target - output) * f'(x)
- Pour les couches cach√©es : Œ¥ = (Œ£ Œ¥_suivant * w) * f'(x)

Les poids sont mis √† jour selon la r√®gle :
```cpp
w_new = w_old + learning_rate * delta * input
```

## üíª Avantages du C++

1. Performance :
   - Ex√©cution plus rapide que Python
   - Gestion directe de la m√©moire
   - Pas d'overhead d'interpr√©teur

2. Possibilit√©s d'Optimisation :
   - Parall√©lisation possible avec OpenMP ou std::thread
   - Vectorisation des calculs
   - Cache-friendly data structures

3. Contr√¥le Total :
   - Pas de magie noire des frameworks
   - Compr√©hension compl√®te du fonctionnement
   - Personnalisation totale possible

## üöÄ Am√©liorations Possibles

1. Parall√©lisation :
   - Parall√©liser le calcul des neurones dans chaque couche
   - Parall√©liser le traitement des batchs pendant l'entra√Ænement
   - Utiliser OpenMP pour une impl√©mentation simple :
   ```cpp
   #pragma omp parallel for
   for (size_t i = 0; i < neurons.size(); i++) {
       // calculs des neurones
   }
   ```

2. Optimisations :
   - Utilisation de SIMD pour les op√©rations vectorielles
   - Batch training pour am√©liorer la convergence
   - Mini-batch stochastic gradient descent

## üìà R√©sultats

Apr√®s entra√Ænement, le r√©seau atteint une pr√©cision excellente :
```
Entr√©e: 0 0 - Sortie: 0.000309991 - Attendu: 0
Entr√©e: 0 1 - Sortie: 0.999058 - Attendu: 1
Entr√©e: 1 0 - Sortie: 0.999065 - Attendu: 1
Entr√©e: 1 1 - Sortie: -0.000622462 - Attendu: 0
```

## üõ† Compilation et Utilisation

```bash
g++ -std=c++11 neural_network.cpp -o neural_network
./neural_network
```

Pour activer les optimisations :
```bash
g++ -std=c++11 -O3 neural_network.cpp -o neural_network
```

Pour la parall√©lisation avec OpenMP :
```bash
g++ -std=c++11 -fopenmp neural_network.cpp -o neural_network
```
